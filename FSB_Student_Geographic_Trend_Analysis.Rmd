---
title: "Cleaning the Combined BI And Survey Career Data"
author: "Allison Riley & Jenna Sayle"
date: "`r format(Sys.time(), '%d %B, %Y')`"
output:
  html_document:
    code_folding: hide
    df_print: paged
    number_sections: yes
    theme: cerulean
    toc: yes
    toc_float: yes
    code_download: true
  word_document:
    toc: no
---

```{r setup, include=FALSE, echo = TRUE}
knitr::opts_chunk$set(echo = TRUE,message=FALSE,warning=FALSE)

#package intialization
rm(list = ls()) # clear global environment
graphics.off() # close all graphics
if(require(pacman)==FALSE) install.packages("pacman")
pacman::p_load(DataExplorer,tidyverse,readxl,zoo,stargazer,kableExtra,skimr,plotly,ggpubr,vtable,tm, gplots, ggplot, httr,stringdist)

```

# Introduction and Purpose

Annually, the FSB conducts a senior survey of graduates to learn of their employment status. In addition, information is verified using LinkedIn and employer survey information. The data you are provided ('FSB_BI_Survey_2019_2021.rds') contains data on graduates for 2019, 2020, and 2021. The data are merged from two sources: the senior survey, and data from the Miami University database.

The product and service this analysis will provide is a soundbite on the geographic trends in FSB placement data. Our clients will utilize our insights to help drive business decisions. For example, Miami University may decide to target companies from a specific geographic area, based on salary or job type to bring to career fair for a specific major. The provided dataset is comprehensive with 42 different attributes per row. This analysis will provide our client with meaningful insights without having to process the dataset. This pain killer will allow our clients to focus their time at work in other areas.

# Business Value Proposition

![](BVP v2.png)

# Data Description

The data file provided is from graduated students who completed this survey from 2019-2021. The data come from two sources, OBIEE and a senior survey.  The files contains 42 variables (columns) with 3,235 observations. There are 4 variables that are factors, 18 variables that are integers or numeric, and 20 variables that are characters. 

1. `nmajor`: numeric,derived, the number of majors
2. `major1`: text, OBIEE, first major
3. `major 2`: text, OBIEE, second major
4. `BBRJ`: binary, OBIEE, an attribute of a student, but we do not know what this stands for
5. `Business Direct Admit`: binary, OBIEE, a direct admit to FSB as a first year
6. `Combined Cacc and Masters`: binary, OBIEE, combined degree student
7. `Dean's List`: binary, OBIEE, achieve dean's list status at least once
8. `First Generation College Stdnt`: binary, OBIEE, first generation student status
9. `FSB Scholars`: binary, OBIEE, FSB scholars program
10. `Honors Program`: binary, OBIEE, member of University honors program
11. `President's list`: binary, OBIEE, achieved president's list at least once
12. `Study Abroud Courtesy Account`: binary, OBIEE, do not know meaning
13. `Transfer Work`: binary, OBIEE, do not know exact meaning
14. `Cum Laude`: binary, OBIEE, graduated Cum Laude
15. `Magna Cum Laude`: binary, OBIEE, graduated Magna Cum Laude
16. `Summa Cum Laude`: binary, OBIEE, graduated Summa Cum Laude
17. `University Honors`: binary, OBIEE, graduated with University Honors
18. `University Honors w/Distinction`: binary, OBIEE, graduated with University Honors with Distinction
19. `minor1`: text, OBIEE, first listed minor
20. `minor2`: text, OBIEE, second listed minor
21. `IPEDS.Race.Ethnicity`: text, OBIEE, race/ethnicity
22. `Gender`: text, OBIEE, sex
23. `GPA.Range`: text, OBIEE, GPA within a .5 range
24. `Term.Code`: numberic, OBIEE, First four digits are the physcal year (beginning in July, e.g. July 2020 is FY 2021). Last two digits is the term (10=fall, 15=winter, 20=spring, 30=summer).
25. `Year.x`: text, derived, first four digits of Term.Code stored as a character variable
26. `latin_honors`: text, survey, latin honors designation
27. `survey_city`: text, survey, student reported city in which their job is located
28. `survey_company`: text, survey, student reported company in which they accepted a job
29. `survey_deptfunc`: text, survey, student reported job function
30. `survey_gradprogram`: text, survey, student reported graduate program they will be attending
31. `survey_gradschool`: text, survey, stuent reported graduate school they will be attending
32. `survey_internfour`: text, survey, student reported fourth internship they held during college
33. `survey_internthree`: text, survey, student reported third internship they held during college
34. `survey_interntwo`: text, survey, student reported second internship they held during college
35. `survey_internone`: text, survey, student reported first internship they held during college
36. `Survey_internships`: text, survey, Student reported number of internships they held during college
37. `survey_offers`: text, survey, student reported number of offers for full time employment received
38. `survey_plans`: text, survey, student reported plans after graduation
39. `survey_pref_field`: text, survey, student reported whether working in preferred field
40. `survey_pref_loc`: text, survey, student reported whether working in preferred location
41. `survey_salary`: numeric, survey, student reported salary
42. `survey_state`: text, survey, student reported state in which job is located

# Data Preprocessing

## Read in the data

```{r, echo = TRUE}
data=readRDS(file = "FSB_BI_Survey_2019_2021.rds")

options(scipen = 999)
```

## Cleaning the Data

```{r}
# Removing those who are "seeking continuing education", "continuing education", "not seeking employment"

ValueToRemove = c("seeking continuing education", "continuing education", "not seeking employment")

data_filtered <- data[!(data$survey_plans %in% ValueToRemove), ]

unique(data_filtered$survey_plans)
```

The first thing in cleaning the data is removing the levels of `survey_plans` that are not needed when evaluating geographic trends of those with a job.  This includes removing those individuals that have selected "seeking continuing education", "continuing education", "not seeking employment". 


```{r}
# Remove the specified columns by their column numbers
data_filtered2 <- data_filtered[, c(2,22,23,25,27,28,29,37,38,39,40,41,42)]
head(data_filtered2)
```

The second step in cleaning the data was to remove all of the unnecessary columns that will not aid in the analysis of geographic trends.  We kept 13 columns from the original 42. 

The next step in cleaning the data was removing unnecessary columns that are not needed when evaluating this dataset.  This will help narrowing down the 42 columns to a limited number of columns to help focus the analysis.

```{r}
plot_missing(data_filtered2)
```

Since we want to look at entries with survey plans listed, missing values for survey plans were removed. 

```{r}
filtered <- subset(data_filtered2, !is.na(survey_plans))
plot_missing(filtered)
```

Once the dataset is finalized, the number of missing values was evaluated.  The first variable that needed to na values to get filtered out was survey_plans.  If they did not indicate what their future plans are, it would be hard to evaluate geographic trends.  It was noticed when looking at the data that some columns have city missing, but not state missing.  For this reason, the missing values for the remaining columns are going to be kept for now.  It may get filtered out later in the analysis.

### Cleaning State, City, and Job

#### Cleaning the State

```{r}
filtered$survey_state <- tolower(filtered$survey_state) # to lower case
filtered$survey_state <- gsub("[^a-zA-Z]", "", filtered$survey_state) # remove abbreviations
filtered$survey_state <- match(filtered$survey_state, tolower(state.abb)) # convert to numerical representations
filtered$survey_state <- state.name[(filtered$survey_state)]
print(unique(filtered$survey_state))
```

When evaluating the data, it was noticed that there were multiple levels for the same state.  For example: OH=ohio-oh=Ohio.  This was cleaned up so there is only 1 level per state listed. 

#### Cleaning the City name

```{r}
filtered$survey_city <- tolower(filtered$survey_city) # to lower case
filtered$survey_city <- gsub("[^a-zA-Z]", "", filtered$survey_city) # remove abbreviations
filtered$survey_city <- gsub("\\s+", "", filtered$survey_city) # remove extra spaces
filtered$survey_city <- gsub(" city$", '' ,  filtered$survey_city) # remove string city on the end 

sort(unique(filtered$survey_city))
```

This task requires some extra cleaning to be done by hand for misspelled cities 
```{r}
# to drop : indiana, dontknowyet, na, tbd, various
filtered$survey_city[filtered$survey_city == "cincinatti"] <- "cincinnati"
filtered$survey_city[filtered$survey_city == "cincinnnati"] <- "cincinnati"
filtered$survey_city[filtered$survey_city == "cincinnatioh"] <- "cincinnati"
filtered$survey_city[filtered$survey_city == 'findlaytentative'] <- 'findlay'
filtered$survey_city[filtered$survey_city == 'findley'] <- 'findlay'
filtered$survey_city[filtered$survey_city == 'ftmyers'] <- 'fortmyers'
filtered$survey_city[filtered$survey_city == 'hamiliton'] <- 'hamilton'
filtered$survey_city[filtered$survey_city == 'milwaukwee'] <- 'milwaukee'
filtered$survey_city[filtered$survey_city == 'newyork'] <- 'newyorkcity'
filtered$survey_city[filtered$survey_city == 'nyc'] <- 'newyorkcity'
filtered$survey_city[filtered$survey_city == 'philadephia'] <- 'philadelphia'
filtered$survey_city[filtered$survey_city == 'springfieldoh'] <- 'springfield'
filtered$survey_city[filtered$survey_city == 'washinton'] <- 'washington'
filtered$survey_city[filtered$survey_city == 'washingtondc'] <- 'washington'
filtered$survey_city[filtered$survey_city == 'westervillecolumbus'] <- 'westerville'
filtered$survey_city[filtered$survey_city == 'witchita'] <- 'wichita'

sort(unique(filtered$survey_city))
```

Thurthermore, drop rows with values for city name that are irrelevant to our analysis : (indiana, dontknowyet, na, tbd, various)

```{r}
# Drop Rows with the following values : indiana, dontknowyet, na, tbd, various

ValueToRemove = c("indiana", "dontknowyet", "na", "tbd", "various", "NA")

filtered <- filtered[!(filtered$survey_city %in% ValueToRemove), ]

sort(unique(filtered$survey_city))
```
Same thing here as to the states. There were multiple levels for the same city.  The city names were cleaned up so there was only 1 level per city. 

#### Retriveing Lattitude and Longitude 
```{r}
survey_city = sort(unique(filtered$survey_city))

# Initialize empty vectors for latitude and longitude
latitude <- numeric(length(survey_city))
longitude <- numeric(length(survey_city))
#print(length(filtered$survey_city)) # Should take about 19 mins to run 

for (i in 1:length(survey_city)) { 
  #print(survey_city[i])
  api_url <- paste0("https://geocode.maps.co/search?q=" , survey_city[i])
  #api_url <- "https://geocode.maps.co/search?q=dfghjkgbjghjgh"
  response <- GET(api_url)
  result <- content(response, "parsed")
  Sys.sleep(0.5)  # Pause for 0.7 seconds
  
  if (is.list(result) && length(result) > 0) {
    # Valid response
    # Process the data
    #print(result[[1]]$lat)
    latitude[i] <- result[[1]]$lat
    #print(result[[1]]$lon)
    longitude[i] <- result[[1]]$lon
  } else {
    # Invalid response due to an HTTP error
    # Handle the HTTP error
    #print("Invalid")
    latitude[i] = NA
    longitude[i] = NA 
    next 
  }

  #break
}
```
##### Apply Latitude and Longitude to Filtered Data Set
```{r}
geoInfo = as.data.frame(cbind(survey_city, latitude, longitude))
print(head(geoInfo))

merged_data <- left_join(filtered, geoInfo, by = "survey_city")
filtered <- merged_data
```


#### Cleaning the Job name
This was a daunting task as there were too many incorrectly entered job titles to comfortably do this by hand. Instead, we created a function that groups entries with similiar names together, under a single job name based on a computed string distance. This method was tested with different thresholds, lower ones being more strict and higher ones being less strict until a threshold = 0.2 was decided on because it correctly clusters names without incorrectly grouping (at the expense of only a few duplicate clusters). 
```{r}
# Load your survey data into a data frame (replace this with your actual data)
survey_data <- filtered
survey_data <- survey_data[complete.cases(survey_data$survey_company), ]

# Create a function to cluster similar company names with emphasis on the first word (case-insensitive)
cluster_similar_names <- function(data, threshold = 0.7) {
  # Convert company names to lowercase
  data$survey_company <- tolower(data$survey_company)
  
  # Extract the first word from each company name and convert to lowercase
  data$first_word <- sapply(strsplit(data$survey_company, " "), function(x) tolower(x[1]))

  # Calculate the string distances between lowercase first words
  dist_matrix <- stringdistmatrix(data$first_word, data$first_word, method = "jw")

  # Create a hierarchical clustering based on string distances
  hclust_result <- hclust(as.dist(dist_matrix))

  # Cut the tree into clusters based on the threshold
  clusters <- cutree(hclust_result, h = threshold)

  # Add the cluster IDs to the original data frame
  data$survey_company_id <- clusters

  # Create a named vector of cluster IDs to the first company name in each cluster
  cluster_names <- sapply(1:max(clusters), function(cluster_id) {
    first_company_in_cluster <- data$survey_company[clusters == cluster_id][1]
    return(first_company_in_cluster)
  })

  # Add the cluster names to the original data frame
  data$survey_company_cluster_name <- cluster_names[clusters]

  # Remove the temporary 'first_word' column if you don't need it
  data$first_word <- NULL

  return(data)
}
```

The threshold was selected after using trial and error. Depending on the threshold will depend on how well the companies match with each other. 

```{r}
# Set a threshold for clustering (adjust as needed)
threshold <- 0.2

# Cluster similar company names with emphasis on the first word (case-insensitive)
clustered_data <- cluster_similar_names(survey_data, threshold)

# Sort and print unique company names before and after clustering
print("Unique Company Names Before Clustering:")
#print(sort(unique(filtered$survey_company)))

CompValueToRemove = c("*startup that cannot be named")

clustered_data <- clustered_data[!(clustered_data$survey_company_cluster_name %in% CompValueToRemove), ]

print("Unique Company Names After Clustering:")
print(head(sort(unique(clustered_data$survey_company_cluster_name))))
```


```{r}
clustered_data <- subset(clustered_data, select = -survey_company_id)
head(clustered_data, n = 10)
```



## Exploratory Data Analysis

This EDA will help to see if there is any additional cleaning that needs to happen. 

```{r}
create_report(clustered_data)
```
When looking at the overall EDA report, we can conclude the following.  

1) the number of missing values that are left are in the "ok" range.  We are going to keep the number of missing values were they are as we know some cities are listed but not their corresponding state.  Additional missing values filtering may be needed, but will be evaluated at that point.  

2) When looking at the histogram for the salary, there is a student who reported a salary of $175,000.  This is considered an outlier given the rest of the salary data is much less.  However, we are going to treat this value as true to what it is.  

3) According to the normal QQ plot and the histogram, the salary data is roughly distributed normally.  With the results of the EDA, we are going to keep the cleaning where it is at to start the analysis.  If we need to go back and clean as we go through the analysis, we will do the necessary cleaning to boost the analysis


## Resulting DataFrame Data Description

With the cleaning of data, removal of unnecessary columns, and added columns below is the data description of the resulting data frame. 

1. `major1`: text, OBIEE, first major
2. `Gender`: text, OBIEE, sex
3. `GPA.Range`: text, OBIEE, GPA within a .5 range
4. `Year.x`: text, derived, first four digits of Term.Code stored as a character variable
5. `survey_city`: text, survey, student reported city in which their job is located
6. `survey_company`: text, survey, student reported company in which they accepted a job
7. `survey_deptfunc`: text, survey, student reported job function
8. `survey_offers`: text, survey, student reported number of offers for full time employment received
9. `survey_plans`: text, survey, student reported plans after graduation
10. `survey_pref_field`: text, survey, student reported whether working in preferred field
11. `survey_pref_loc`: text, survey, student reported whether working in preferred location
12. `survey_salary`: numeric, survey, student reported salary
13. `survey_state`: text, survey, student reported state in which job is located
14. `latitude`: Latitude of city
15. `longitude`: Longitude of city
16. `survey_company_cluster_name`: Cleaned `survey_company` column


##### R Version results were produced in
```{r, echo = FALSE}
R.version
write.csv(clustered_data, file = "FinalCleanedData.csv", row.names = FALSE)
```

