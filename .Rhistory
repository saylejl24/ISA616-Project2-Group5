cluster_similar_names <- function(data, threshold = 0.2) {
# Calculate the string distances between company names
dist_matrix <- stringdistmatrix(data$survey_company, data$survey_company, method = "jw")
# Create a hierarchical clustering based on string distances
hclust_result <- hclust(as.dist(dist_matrix))
# Cut the tree into clusters based on the threshold
clusters <- cutree(hclust_result, h = threshold)
# Add the cluster IDs to the original data frame
data$survey_company_id <- clusters
return(data)
}
# Set a threshold for clustering (adjust as needed)
threshold <- 0.5
# Cluster similar company names
clustered_data <- cluster_similar_names(survey_data, threshold)
sort(unique(filtered$survey_company))
filtered$survey_company <- tolower(filtered$survey_company) # to lower case
filtered$survey_company[filtered$survey_company == "84.51ยบ"] <- "84.51"
filtered$survey_company[filtered$survey_company == "abbott nutrition"] <- "abbott"
filtered$survey_company[filtered$survey_company == "abercrombie"] <- "abercrombie & fitch"
filtered$survey_company[filtered$survey_company == "aldi us" | filtered$survey_company == "aldi usa"] <- "aldi"
filtered$survey_company[filtered$survey_company == "american eagle outfitters" | filtered$survey_company == "american eagle outfitters, Inc."] <- "american eagle"
filtered$survey_company[filtered$survey_company == "amazon web services"] <- "amazon"
filtered$survey_company[filtered$survey_company == "american electric power (aep)"] <- "american electric power"
filtered$survey_company[filtered$survey_company == "ameriprise financial - redstone wealth advisors"] <- "ameriprise financial services"
filtered$survey_company[filtered$survey_company == "andersen"] <- "andersen tax"
filtered$survey_company[filtered$survey_company == "ankura"] <- "ankura consulting"
filtered$survey_company[filtered$survey_company == "ayco, a goldman sachs company"] <- "ayco"
filtered$survey_company[filtered$survey_company == "barclays"] <- "barclays investment bank"
filtered$survey_company[filtered$survey_company == "bath & body works"] <- "bath and body works"
filtered$survey_company[filtered$survey_company == "baxter"] <- "baxter international"
filtered$survey_company[filtered$survey_company == "bdo usa" | filtered$survey_company == "bdo usa llp"] <- "bdo"
knitr::opts_chunk$set(echo = TRUE,message=FALSE,warning=FALSE)
#package intialization
rm(list = ls()) # clear global environment
graphics.off() # close all graphics
if(require(pacman)==FALSE) install.packages("pacman")
pacman::p_load(DataExplorer,tidyverse,readxl,zoo,stargazer,kableExtra,skimr,plotly,ggpubr,vtable,tm, gplots, ggplot)
data=readRDS(file = "FSB_BI_Survey_2019_2021.rds")
options(scipen = 999)
# Removing those who are "seeking continuing education", "continuing education", "not seeking employment"
ValueToRemove = c("seeking continuing education", "continuing education", "not seeking employment")
data_filtered <- data[!(data$survey_plans %in% ValueToRemove), ]
unique(data_filtered$survey_plans)
# Remove the specified columns by their column numbers
data_filtered2 <- data_filtered[, c(2,3,22,23,25,27,28,29,37,38,39,40,41,42)]
head(data_filtered2)
colSums(is.na(data_filtered2))
plot_missing(data_filtered2)
filtered <- subset(data_filtered2, !is.na(survey_plans))
plot_missing(filtered)
filtered$survey_state <- tolower(filtered$survey_state) # to lower case
filtered$survey_state <- gsub("[^a-zA-Z]", "", filtered$survey_state) # remove abbreviations
filtered$survey_state <- match(filtered$survey_state, tolower(state.abb)) # convert to numerical representations
filtered$survey_state <- state.name[(filtered$survey_state)]
print(unique(filtered$survey_state))
filtered$survey_city <- tolower(filtered$survey_city) # to lower case
filtered$survey_city <- gsub("[^a-zA-Z]", "", filtered$survey_city) # remove abbreviations
filtered$survey_city <- gsub("\\s+", "", filtered$survey_city) # remove extra spaces
filtered$survey_city <- gsub(" city$", '' ,  filtered$survey_city) # remove string city on the end
sort(unique(filtered$survey_city))
# to drop : indiana, dontknowyet, na, tbd, various
filtered$survey_city[filtered$survey_city == "cincinatti"] <- "cincinnati"
filtered$survey_city[filtered$survey_city == "cincinnnati"] <- "cincinnati"
filtered$survey_city[filtered$survey_city == "cincinnatioh"] <- "cincinnati"
filtered$survey_city[filtered$survey_city == 'findlaytentative'] <- 'findlay'
filtered$survey_city[filtered$survey_city == 'findley'] <- 'findlay'
filtered$survey_city[filtered$survey_city == 'ftmyers'] <- 'fortmyers'
filtered$survey_city[filtered$survey_city == 'hamiliton'] <- 'hamilton'
filtered$survey_city[filtered$survey_city == 'milwaukwee'] <- 'milwaukee'
filtered$survey_city[filtered$survey_city == 'newyork'] <- 'newyorkcity'
filtered$survey_city[filtered$survey_city == 'nyc'] <- 'newyorkcity'
filtered$survey_city[filtered$survey_city == 'philadephia'] <- 'philadelphia'
filtered$survey_city[filtered$survey_city == 'springfieldoh'] <- 'springfield'
filtered$survey_city[filtered$survey_city == 'washinton'] <- 'washington'
filtered$survey_city[filtered$survey_city == 'washingtondc'] <- 'washington'
filtered$survey_city[filtered$survey_city == 'westervillecolumbus'] <- 'westerville'
filtered$survey_city[filtered$survey_city == 'witchita'] <- 'wichita'
sort(unique(filtered$survey_city))
# Drop Rows with the following values : indiana, dontknowyet, na, tbd, various
ValueToRemove = c("indiana", "dontknowyet", "na", "tbd", "various", "NA")
filtered <- filtered[!(filtered$survey_city %in% ValueToRemove), ]
sort(unique(filtered$survey_city))
create_report(filtered)
max(filtered$survey_salary)
summary(filtered$survey_salary)
summary(data)
str(data)
knitr::opts_chunk$set(echo = TRUE,message=FALSE,warning=FALSE)
#package intialization
rm(list = ls()) # clear global environment
graphics.off() # close all graphics
if(require(pacman)==FALSE) install.packages("pacman")
pacman::p_load(DataExplorer,tidyverse,readxl,zoo,stargazer,kableExtra,skimr,plotly,ggpubr,vtable,tm, gplots, ggplot)
data=readRDS(file = "FSB_BI_Survey_2019_2021.rds")
options(scipen = 999)
# Removing those who are "seeking continuing education", "continuing education", "not seeking employment"
ValueToRemove = c("seeking continuing education", "continuing education", "not seeking employment")
data_filtered <- data[!(data$survey_plans %in% ValueToRemove), ]
unique(data_filtered$survey_plans)
# Remove the specified columns by their column numbers
data_filtered2 <- data_filtered[, c(2,3,22,23,25,27,28,29,37,38,39,40,41,42)]
head(data_filtered2)
levels(data_filtered2$major2)
count(levels(data_filtered2$major2))
table(data_filtered2$major2)
View(data)
# Remove the specified columns by their column numbers
data_filtered2 <- data_filtered[, c(2,22,23,25,27,28,29,37,38,39,40,41,42)]
head(data_filtered2)
R.version
write.csv(clustered_data, file = "FinalCleanedData.csv", row.names = FALSE)
knitr::opts_chunk$set(echo = TRUE,message=FALSE,warning=FALSE)
#package intialization
rm(list = ls()) # clear global environment
graphics.off() # close all graphics
if(require(pacman)==FALSE) install.packages("pacman")
pacman::p_load(DataExplorer,tidyverse,readxl,zoo,stargazer,kableExtra,skimr,plotly,ggpubr,vtable,tm, gplots, ggplot)
data=readRDS(file = "FSB_BI_Survey_2019_2021.rds")
options(scipen = 999)
# Removing those who are "seeking continuing education", "continuing education", "not seeking employment"
ValueToRemove = c("seeking continuing education", "continuing education", "not seeking employment")
data_filtered <- data[!(data$survey_plans %in% ValueToRemove), ]
unique(data_filtered$survey_plans)
# Remove the specified columns by their column numbers
data_filtered2 <- data_filtered[, c(2,22,23,25,27,28,29,37,38,39,40,41,42)]
head(data_filtered2)
plot_missing(data_filtered2)
filtered <- subset(data_filtered2, !is.na(survey_plans))
plot_missing(filtered)
filtered$survey_state <- tolower(filtered$survey_state) # to lower case
filtered$survey_state <- gsub("[^a-zA-Z]", "", filtered$survey_state) # remove abbreviations
filtered$survey_state <- match(filtered$survey_state, tolower(state.abb)) # convert to numerical representations
filtered$survey_state <- state.name[(filtered$survey_state)]
print(unique(filtered$survey_state))
filtered$survey_city <- tolower(filtered$survey_city) # to lower case
filtered$survey_city <- gsub("[^a-zA-Z]", "", filtered$survey_city) # remove abbreviations
filtered$survey_city <- gsub("\\s+", "", filtered$survey_city) # remove extra spaces
filtered$survey_city <- gsub(" city$", '' ,  filtered$survey_city) # remove string city on the end
#sort(unique(filtered$survey_city))
# to drop : indiana, dontknowyet, na, tbd, various
filtered$survey_city[filtered$survey_city == "cincinatti"] <- "cincinnati"
filtered$survey_city[filtered$survey_city == "cincinnnati"] <- "cincinnati"
filtered$survey_city[filtered$survey_city == "cincinnatioh"] <- "cincinnati"
filtered$survey_city[filtered$survey_city == 'findlaytentative'] <- 'findlay'
filtered$survey_city[filtered$survey_city == 'findley'] <- 'findlay'
filtered$survey_city[filtered$survey_city == 'ftmyers'] <- 'fortmyers'
filtered$survey_city[filtered$survey_city == 'hamiliton'] <- 'hamilton'
filtered$survey_city[filtered$survey_city == 'milwaukwee'] <- 'milwaukee'
filtered$survey_city[filtered$survey_city == 'newyork'] <- 'newyorkcity'
filtered$survey_city[filtered$survey_city == 'nyc'] <- 'newyorkcity'
filtered$survey_city[filtered$survey_city == 'philadephia'] <- 'philadelphia'
filtered$survey_city[filtered$survey_city == 'springfieldoh'] <- 'springfield'
filtered$survey_city[filtered$survey_city == 'washinton'] <- 'washington'
filtered$survey_city[filtered$survey_city == 'washingtondc'] <- 'washington'
filtered$survey_city[filtered$survey_city == 'westervillecolumbus'] <- 'westerville'
filtered$survey_city[filtered$survey_city == 'witchita'] <- 'wichita'
#sort(unique(filtered$survey_city))
# Drop Rows with the following values : indiana, dontknowyet, na, tbd, various
ValueToRemove = c("indiana", "dontknowyet", "na", "tbd", "various", "NA")
filtered <- filtered[!(filtered$survey_city %in% ValueToRemove), ]
sort(unique(filtered$survey_city))
library(stringdist)
# Load your survey data into a data frame (replace this with your actual data)
survey_data <- filtered
survey_data <- survey_data[complete.cases(survey_data$survey_company), ]
# Create a function to cluster similar company names with emphasis on the first word (case-insensitive)
cluster_similar_names <- function(data, threshold = 0.7) {
# Convert company names to lowercase
data$survey_company <- tolower(data$survey_company)
# Extract the first word from each company name and convert to lowercase
data$first_word <- sapply(strsplit(data$survey_company, " "), function(x) tolower(x[1]))
# Calculate the string distances between lowercase first words
dist_matrix <- stringdistmatrix(data$first_word, data$first_word, method = "jw")
# Create a hierarchical clustering based on string distances
hclust_result <- hclust(as.dist(dist_matrix))
# Cut the tree into clusters based on the threshold
clusters <- cutree(hclust_result, h = threshold)
# Add the cluster IDs to the original data frame
data$survey_company_id <- clusters
# Create a named vector of cluster IDs to the first company name in each cluster
cluster_names <- sapply(1:max(clusters), function(cluster_id) {
first_company_in_cluster <- data$survey_company[clusters == cluster_id][1]
return(first_company_in_cluster)
})
# Add the cluster names to the original data frame
data$survey_company_cluster_name <- cluster_names[clusters]
# Remove the temporary 'first_word' column if you don't need it
data$first_word <- NULL
return(data)
}
# Set a threshold for clustering (adjust as needed)
threshold <- 0.2
# Cluster similar company names with emphasis on the first word (case-insensitive)
clustered_data <- cluster_similar_names(survey_data, threshold)
# Sort and print unique company names before and after clustering
print("Unique Company Names Before Clustering:")
#print(sort(unique(filtered$survey_company)))
print("Unique Company Names After Clustering:")
print(sort(unique(clustered_data$survey_company_cluster_name)))
clustered_data <- subset(clustered_data, select = -survey_company_id)
head(clustered_data, n = 10)
create_report(clustered_data)
R.version
write.csv(clustered_data, file = "FinalCleanedData.csv", row.names = FALSE)
knitr::opts_chunk$set(echo = TRUE,message=FALSE,warning=FALSE)
#package intialization
if(require(pacman)==FALSE) install.packages("pacman")
pacman::p_load(tidyverse, #for data manipulation
plotly, #for interactive plots
flexdashboard, #to make a dashboard
fontawesome, #for icons
DT #for better tables
)
#read in data
geographic_trends_data=read_csv("FinalCleanedData.csv")
levels(clustered_data$survey_deptfunc)
levels(data$survey_deptfunc)
unique(data$survey_deptfunc)
knitr::opts_chunk$set(echo = TRUE,message=FALSE,warning=FALSE)
#package intialization
if(require(pacman)==FALSE) install.packages("pacman")
pacman::p_load(tidyverse, #for data manipulation
plotly, #for interactive plots
flexdashboard, #to make a dashboard
fontawesome, #for icons
DT,
ggplot2,
maps,
ggrepel, #for better tables
dplyr,
uscities
)
#read in data
geographic_trends_data=read_csv("FinalCleanedData.csv")
# fix NYC coordinates
# Define the true coordinates of New York City
nyc_latitude <- 40.7128
nyc_longitude <- -74.0060
# Fill the rows where survey_city is 'newyorkcity' with the true coordinates
geographic_trends_data <- geographic_trends_data %>%
mutate(latitude = ifelse(survey_city == 'newyorkcity', nyc_latitude, latitude),
longitude = ifelse(survey_city == 'newyorkcity', nyc_longitude, longitude))
top_cities <- geographic_trends_data %>%
group_by(survey_city) %>%
summarize(listing_count = n()) %>%
arrange(desc(listing_count)) %>%
top_n(10, listing_count)
# Subset the geographic_trends_data to only include the top 10 cities
geographic_trends_subset <- geographic_trends_data %>%
filter(survey_city %in% top_cities$survey_city)
# Get a map of the United States
us_map <- map_data("world")
ggplot() +
geom_polygon(data = us_map, aes(x=long, y = lat, group = group), fill="black", alpha=0.1) +
geom_path(data = us_map, aes(x = long, y = lat, group = group), color = "gray", size = 0.5) +  # Add state borders
geom_point(data = geographic_trends_data, aes(x = longitude, y = latitude, alpha = 0.2), color = 'purple') +
coord_cartesian(xlim = c(-110, -65), ylim = c(25, 50) ) +
geom_text(data = geographic_trends_subset, aes(x = longitude, y = latitude, label = survey_city), hjust = 0.5, vjust = 0.5, size = 2.7) +
scale_alpha(guide = "none") +
theme_bw()
# fix NYC coordinates
# Define the true coordinates of New York City
nyc_latitude <- 40.7128
nyc_longitude <- -74.0060
# Fill the rows where survey_city is 'newyorkcity' with the true coordinates
geographic_trends_data <- geographic_trends_data %>%
mutate(latitude = ifelse(survey_city == 'newyorkcity', nyc_latitude, latitude),
longitude = ifelse(survey_city == 'newyorkcity', nyc_longitude, longitude))
top_cities <- geographic_trends_data %>%
group_by(survey_city) %>%
summarize(listing_count = n()) %>%
arrange(desc(listing_count)) %>%
top_n(10, listing_count)
# Subset the geographic_trends_data to only include the top 10 cities
geographic_trends_subset <- geographic_trends_data %>%
filter(survey_city %in% top_cities$survey_city)
# Get a map of the United States
us_map <- map_data("world")
ggplot() +
geom_polygon(data = us_map, aes(x=long, y = lat, group = group), fill="black", alpha=0.1) +
geom_path(data = us_map, aes(x = long, y = lat, group = group), color = "gray", size = 0.5) +  # Add state borders
geom_point(data = geographic_trends_data, aes(x = longitude, y = latitude, alpha = 0.2), color = 'purple') +
coord_cartesian(xlim = c(-110, -65), ylim = c(25, 50) ) +
geom_text(data = geographic_trends_subset, aes(x = longitude, y = latitude, label = survey_city), hjust = 0.5, vjust = 0.5, size = 2.7) +
scale_alpha(guide = "none") +
theme_minimal()
# fix NYC coordinates
# Define the true coordinates of New York City
nyc_latitude <- 40.7128
nyc_longitude <- -74.0060
# Fill the rows where survey_city is 'newyorkcity' with the true coordinates
geographic_trends_data <- geographic_trends_data %>%
mutate(latitude = ifelse(survey_city == 'newyorkcity', nyc_latitude, latitude),
longitude = ifelse(survey_city == 'newyorkcity', nyc_longitude, longitude))
top_cities <- geographic_trends_data %>%
group_by(survey_city) %>%
summarize(listing_count = n()) %>%
arrange(desc(listing_count)) %>%
top_n(10, listing_count)
# Subset the geographic_trends_data to only include the top 10 cities
geographic_trends_subset <- geographic_trends_data %>%
filter(survey_city %in% top_cities$survey_city)
# Get a map of the United States
us_map <- map_data("world")
ggplot() +
geom_polygon(data = us_map, aes(x=long, y = lat, group = group), fill="black", alpha=0.1) +
geom_path(data = us_map, aes(x = long, y = lat, group = group), color = "gray", size = 0.5) +  # Add state borders
geom_point(data = geographic_trends_data, aes(x = longitude, y = latitude, alpha = 0.2), color = 'purple') +
coord_cartesian(xlim = c(-110, -65), ylim = c(25, 50) ) +
geom_text(data = geographic_trends_subset, aes(x = longitude, y = latitude, label = survey_city), hjust = 0.5, vjust = 0.5, size = 2.7) +
scale_alpha(guide = "none") +
theme(panel.background = element_rect(fill = "white")) +
theme_minimal()
# fix NYC coordinates
# Define the true coordinates of New York City
nyc_latitude <- 40.7128
nyc_longitude <- -74.0060
# Fill the rows where survey_city is 'newyorkcity' with the true coordinates
geographic_trends_data <- geographic_trends_data %>%
mutate(latitude = ifelse(survey_city == 'newyorkcity', nyc_latitude, latitude),
longitude = ifelse(survey_city == 'newyorkcity', nyc_longitude, longitude))
top_cities <- geographic_trends_data %>%
group_by(survey_city) %>%
summarize(listing_count = n()) %>%
arrange(desc(listing_count)) %>%
top_n(10, listing_count)
# Subset the geographic_trends_data to only include the top 10 cities
geographic_trends_subset <- geographic_trends_data %>%
filter(survey_city %in% top_cities$survey_city)
# Get a map of the United States
us_map <- map_data("world")
ggplot() +
geom_polygon(data = us_map, aes(x=long, y = lat, group = group), fill="black", alpha=0.1) +
geom_path(data = us_map, aes(x = long, y = lat, group = group), color = "gray", size = 0.5) +  # Add state borders
geom_point(data = geographic_trends_data, aes(x = longitude, y = latitude, alpha = 0.2), color = 'purple') +
coord_cartesian(xlim = c(-110, -65), ylim = c(25, 50) ) +
geom_text(data = geographic_trends_subset, aes(x = longitude, y = latitude, label = survey_city), hjust = 0.5, vjust = 0.5, size = 2.7) +
scale_alpha(guide = "none") +
theme(panel.background = element_rect(fill = "white"))
# fix NYC coordinates
# Define the true coordinates of New York City
nyc_latitude <- 40.7128
nyc_longitude <- -74.0060
# Fill the rows where survey_city is 'newyorkcity' with the true coordinates
geographic_trends_data <- geographic_trends_data %>%
mutate(latitude = ifelse(survey_city == 'newyorkcity', nyc_latitude, latitude),
longitude = ifelse(survey_city == 'newyorkcity', nyc_longitude, longitude))
top_cities <- geographic_trends_data %>%
group_by(survey_city) %>%
summarize(listing_count = n()) %>%
arrange(desc(listing_count)) %>%
top_n(10, listing_count)
# Subset the geographic_trends_data to only include the top 10 cities
geographic_trends_subset <- geographic_trends_data %>%
filter(survey_city %in% top_cities$survey_city)
# Get a map of the United States
us_map <- map_data("world")
ggplot() +
geom_polygon(data = us_map, aes(x=long, y = lat, group = group), fill="black", alpha=0.1) +
geom_path(data = us_map, aes(x = long, y = lat, group = group), color = "gray", size = 0.5) +  # Add state borders
geom_point(data = geographic_trends_data, aes(x = longitude, y = latitude, alpha = 0.2), color = 'purple') +
coord_cartesian(xlim = c(-110, -65), ylim = c(25, 50) ) +
geom_text(data = geographic_trends_subset, aes(x = longitude, y = latitude, label = survey_city), hjust = 0.5, vjust = 0.5, size = 2.7) +
scale_alpha(guide = "none") +
theme_minimal()
# fix NYC coordinates
# Define the true coordinates of New York City
nyc_latitude <- 40.7128
nyc_longitude <- -74.0060
# Fill the rows where survey_city is 'newyorkcity' with the true coordinates
geographic_trends_data <- geographic_trends_data %>%
mutate(latitude = ifelse(survey_city == 'newyorkcity', nyc_latitude, latitude),
longitude = ifelse(survey_city == 'newyorkcity', nyc_longitude, longitude))
top_cities <- geographic_trends_data %>%
group_by(survey_city) %>%
summarize(listing_count = n()) %>%
arrange(desc(listing_count)) %>%
top_n(10, listing_count)
# Subset the geographic_trends_data to only include the top 10 cities
geographic_trends_subset <- geographic_trends_data %>%
filter(survey_city %in% top_cities$survey_city)
# Get a map of the United States
us_map <- map_data("world")
ggplot() +
geom_polygon(data = us_map, aes(x=long, y = lat, group = group), fill="black", alpha=0.1) +
geom_path(data = us_map, aes(x = long, y = lat, group = group), color = "gray", size = 0.5) +  # Add state borders
geom_point(data = geographic_trends_data, aes(x = longitude, y = latitude, alpha = 0.2), color = 'purple') +
coord_cartesian(xlim = c(-110, -65), ylim = c(25, 50) ) +
geom_text(data = geographic_trends_subset, aes(x = longitude, y = latitude, label = survey_city), hjust = 0.5, vjust = 0.5, size = 5) +
scale_alpha(guide = "none") +
theme_minimal()
# fix NYC coordinates
# Define the true coordinates of New York City
nyc_latitude <- 40.7128
nyc_longitude <- -74.0060
# Fill the rows where survey_city is 'newyorkcity' with the true coordinates
geographic_trends_data <- geographic_trends_data %>%
mutate(latitude = ifelse(survey_city == 'newyorkcity', nyc_latitude, latitude),
longitude = ifelse(survey_city == 'newyorkcity', nyc_longitude, longitude))
top_cities <- geographic_trends_data %>%
group_by(survey_city) %>%
summarize(listing_count = n()) %>%
arrange(desc(listing_count)) %>%
top_n(10, listing_count)
# Subset the geographic_trends_data to only include the top 10 cities
geographic_trends_subset <- geographic_trends_data %>%
filter(survey_city %in% top_cities$survey_city)
# Get a map of the United States
us_map <- map_data("world")
ggplot() +
geom_polygon(data = us_map, aes(x=long, y = lat, group = group), fill="black", alpha=0.1) +
geom_path(data = us_map, aes(x = long, y = lat, group = group), color = "gray", size = 0.5) +  # Add state borders
geom_point(data = geographic_trends_data, aes(x = longitude, y = latitude, alpha = 0.2), color = 'purple') +
coord_cartesian(xlim = c(-110, -65), ylim = c(25, 50) ) +
geom_text(data = geographic_trends_subset, aes(x = longitude, y = latitude, label = survey_city), hjust = 0.5, vjust = 0.5, size = 2.5) +
scale_alpha(guide = "none") +
theme_minimal()
# fix NYC coordinates
# Define the true coordinates of New York City
nyc_latitude <- 40.7128
nyc_longitude <- -74.0060
# Fill the rows where survey_city is 'newyorkcity' with the true coordinates
geographic_trends_data <- geographic_trends_data %>%
mutate(latitude = ifelse(survey_city == 'newyorkcity', nyc_latitude, latitude),
longitude = ifelse(survey_city == 'newyorkcity', nyc_longitude, longitude))
top_cities <- geographic_trends_data %>%
group_by(survey_city) %>%
summarize(listing_count = n()) %>%
arrange(desc(listing_count)) %>%
top_n(10, listing_count)
# Subset the geographic_trends_data to only include the top 10 cities
geographic_trends_subset <- geographic_trends_data %>%
filter(survey_city %in% top_cities$survey_city)
# Get a map of the United States
us_map <- map_data("world")
ggplot() +
geom_polygon(data = us_map, aes(x=long, y = lat, group = group), fill="black", alpha=0.1) +
geom_path(data = us_map, aes(x = long, y = lat, group = group), color = "gray", size = 0.5) +  # Add state borders
geom_point(data = geographic_trends_data, aes(x = longitude, y = latitude, alpha = 0.2), color = 'purple') +
coord_cartesian(xlim = c(-110, -65), ylim = c(25, 50) ) +
geom_text(data = geographic_trends_subset, aes(x = longitude, y = latitude, label = survey_city), hjust = 0.5, vjust = 0.5, size = 2.5) +
scale_alpha(guide = "none") +
scale_fill_identity() +
theme_minimal()
# fix NYC coordinates
# Define the true coordinates of New York City
nyc_latitude <- 40.7128
nyc_longitude <- -74.0060
# Fill the rows where survey_city is 'newyorkcity' with the true coordinates
geographic_trends_data <- geographic_trends_data %>%
mutate(latitude = ifelse(survey_city == 'newyorkcity', nyc_latitude, latitude),
longitude = ifelse(survey_city == 'newyorkcity', nyc_longitude, longitude))
top_cities <- geographic_trends_data %>%
group_by(survey_city) %>%
summarize(listing_count = n()) %>%
arrange(desc(listing_count)) %>%
top_n(10, listing_count)
# Subset the geographic_trends_data to only include the top 10 cities
geographic_trends_subset <- geographic_trends_data %>%
filter(survey_city %in% top_cities$survey_city)
# Get a map of the United States
us_map <- map_data("world")
ggplot() +
geom_polygon(data = us_map, aes(x=long, y = lat, group = group), fill="black", alpha=0.1) +
geom_path(data = us_map, aes(x = long, y = lat, group = group), color = "gray", size = 0.5) +  # Add state borders
geom_point(data = geographic_trends_data, aes(x = longitude, y = latitude, alpha = 0.2), color = 'purple') +
coord_cartesian(xlim = c(-110, -65), ylim = c(25, 50) ) +
geom_text(data = geographic_trends_subset, aes(x = longitude, y = latitude, label = survey_city), hjust = 0.5, vjust = 0.5, size = 2.5) +
scale_alpha(guide = "none") +
theme_minimal() +
scale_fill_identity()
ggplot() +
geom_polygon(data = us_map, aes(x=long, y = lat, group = group), fill="black", alpha=0.1) +
geom_path(data = us_map, aes(x = long, y = lat, group = group), color = "gray", size = 0.5) +  # Add state borders
geom_point(data = geographic_trends_data, aes(x = longitude, y = latitude, alpha = 0.2), color = 'purple') +
coord_cartesian(xlim = c(-110, -65), ylim = c(25, 50) ) +
geom_text(data = geographic_trends_subset, aes(x = longitude, y = latitude, label = survey_city), hjust = 0.2, vjust = 0.5, size = 2.5) +
scale_alpha(guide = "none") +
theme_minimal()
ggplot() +
geom_polygon(data = us_map, aes(x=long, y = lat, group = group), fill="black", alpha=0.1) +
geom_path(data = us_map, aes(x = long, y = lat, group = group), color = "gray", size = 0.5) +  # Add state borders
geom_point(data = geographic_trends_data, aes(x = longitude, y = latitude, alpha = 0.2), color = 'purple') +
coord_cartesian(xlim = c(-110, -65), ylim = c(25, 50) ) +
geom_text(data = geographic_trends_subset, aes(x = longitude, y = latitude, label = survey_city), hjust = 0.1, vjust = 0.5, size = 2.5) +
scale_alpha(guide = "none") +
theme_minimal()
ggplot() +
geom_polygon(data = us_map, aes(x=long, y = lat, group = group), fill="black", alpha=0.1) +
geom_path(data = us_map, aes(x = long, y = lat, group = group), color = "gray", size = 0.5) +  # Add state borders
geom_point(data = geographic_trends_data, aes(x = longitude, y = latitude, alpha = 0.2), color = 'purple') +
coord_cartesian(xlim = c(-110, -65), ylim = c(25, 50) ) +
geom_text(data = geographic_trends_subset, aes(x = longitude, y = latitude, label = survey_city), hjust = 0.1, vjust = 0.3, size = 2.5) +
scale_alpha(guide = "none") +
theme_minimal()
ggplot() +
geom_polygon(data = us_map, aes(x=long, y = lat, group = group), fill="black", alpha=0.1) +
geom_path(data = us_map, aes(x = long, y = lat, group = group), color = "gray", size = 0.5) +  # Add state borders
geom_point(data = geographic_trends_data, aes(x = longitude, y = latitude, alpha = 0.2), color = 'purple') +
coord_cartesian(xlim = c(-110, -65), ylim = c(25, 50) ) +
geom_text(data = geographic_trends_subset, aes(x = longitude, y = latitude, label = survey_city), hjust = 0.1, vjust = 0.1, size = 2.5) +
scale_alpha(guide = "none") +
theme_minimal()
ggplot() +
geom_polygon(data = us_map, aes(x=long, y = lat, group = group), fill="white", alpha=0.1) +
geom_path(data = us_map, aes(x = long, y = lat, group = group), color = "gray", size = 0.5) +  # Add state borders
geom_point(data = geographic_trends_data, aes(x = longitude, y = latitude, alpha = 0.2), color = 'purple') +
coord_cartesian(xlim = c(-110, -65), ylim = c(25, 50) ) +
geom_text(data = geographic_trends_subset, aes(x = longitude, y = latitude, label = survey_city), hjust = 0.1, vjust = 0.1, size = 2.5) +
scale_alpha(guide = "none") +
theme_minimal()
