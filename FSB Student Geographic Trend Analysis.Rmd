---
title: "Cleaning the Combined BI And Survey Career Data"
author: "Allison Riley & Jenna Sayle"
output:
  html_document:
    date: "`r format(Sys.time(), '%d %B, %Y')`"
    code_folding: hide
    df_print: paged
    number_sections: yes
    theme: cerulean
    toc: yes
    toc_float: yes
    code_download: true
  word_document:
    toc: no
---

```{r setup, include=FALSE, echo = TRUE}
knitr::opts_chunk$set(echo = TRUE,message=FALSE,warning=FALSE)

#package intialization
rm(list = ls()) # clear global environment
graphics.off() # close all graphics
if(require(pacman)==FALSE) install.packages("pacman")
pacman::p_load(DataExplorer,tidyverse,readxl,zoo,stargazer,kableExtra,skimr,plotly,ggpubr,vtable,tm, gplots, ggplot)
```

# Introduction and Purpose

Annually, the FSB conducts a senior survey of graduates to learn of their employment status. In addition, information is verified using LinkedIn and employer survey information. The data you are provided ('FSB_BI_Survey_2019_2021.rds') contains data on graduates for 2019, 2020, and 2021. The data are merged from two sources: the senior survey, and data from the Miami University database.

The product and service this analysis will provide is a soundbite on the geographic trends in FSB placement data. Our clients will utilize our insights to help drive business decisions. For example, Miami University may decide to target companies from a specific geographic area, based on salary or job type to bring to career fair for a specific major. The provided dataset is comprehensive with 42 different attributes per row. This analysis will provide our client with meaningful insights without having to process the dataset. This pain killer will allow our clients to focus their time at work in other areas.

# Business Value Proposition

![](BVP v2.png)

# Data Sources

The data file provided is from graduated students who completed this survey from 2019-2021.

## Read in the data

```{r, echo = TRUE}
data=readRDS(file = "FSB_BI_Survey_2019_2021.rds")

options(scipen = 999)
```

# Data Preprocessing

## Cleaning the Data

```{r}
# Removing those who are "seeking continuing education", "continuing education", "not seeking employment", NA

ValueToRemove = c("seeking continuing education", "continuing education", "not seeking employment", "NA")

data_filtered <- data[!(data$survey_plans %in% ValueToRemove), ]

unique(data_filtered$survey_plans)
```

Remove unnecessary columns

```{r}
# Remove the specified columns by their column numbers
data_filtered2 <- data_filtered[, c(2,3,22,23,25,27,28,29,37,38,39,40,41,42)]
head(data_filtered2)
```

Look at missing (drop row if needed)
```{r}
colSums(is.na(data_filtered2))
```

Omit NA values in survey_plans
```{r}
filtered <- subset(data_filtered2, !is.na(survey_plans))
colSums(is.na(filtered))
```

Fix text issues in State
```{r}

filtered$survey_state <- tolower(filtered$survey_state) # to lower case
filtered$survey_state <- gsub("[^a-zA-Z]", "", filtered$survey_state) # remove abbreviations
filtered$survey_state <- match(filtered$survey_state, tolower(state.abb)) # convert to numerical representations
filtered$survey_state <- state.name[(filtered$survey_state)]
print(unique(filtered$survey_state))
```

Fix text issues in City

```{r}
filtered$survey_city <- tolower(filtered$survey_city) # to lower case
filtered$survey_city <- gsub("[^a-zA-Z]", "", filtered$survey_city) # remove abbreviations
filtered$survey_city <- gsub("\\s+", "", filtered$survey_city) # remove extra spaces
filtered$survey_city <- gsub(" city$", '' ,  filtered$survey_city) # remove string city on the end 

print(sort(unique(filtered$survey_city)))
```

Fix small errors in City by hand 

```{r}
# to drop : indiana, dontknowyet, na, tbd, various

filtered$survey_city[filtered$survey_city == "cincinatti"] <- "cincinnati"
filtered$survey_city[filtered$survey_city == "cincinnnati"] <- "cincinnati"
filtered$survey_city[filtered$survey_city == "cincinnatioh"] <- "cincinnati"
filtered$survey_city[filtered$survey_city == 'findlaytentative'] <- 'findlay'
filtered$survey_city[filtered$survey_city == 'findley'] <- 'findlay'
filtered$survey_city[filtered$survey_city == 'ftmyers'] <- 'fortmyers'
filtered$survey_city[filtered$survey_city == 'hamiliton'] <- 'hamilton'
filtered$survey_city[filtered$survey_city == 'milwaukwee'] <- 'milwaukee'
filtered$survey_city[filtered$survey_city == 'newyork'] <- 'newyorkcity'
filtered$survey_city[filtered$survey_city == 'nyc'] <- 'newyorkcity'
filtered$survey_city[filtered$survey_city == 'philadephia'] <- 'philadelphia'
filtered$survey_city[filtered$survey_city == 'springfieldoh'] <- 'springfield'
filtered$survey_city[filtered$survey_city == 'washinton'] <- 'washington'
filtered$survey_city[filtered$survey_city == 'washingtondc'] <- 'washington'
filtered$survey_city[filtered$survey_city == 'westervillecolumbus'] <- 'westerville'
filtered$survey_city[filtered$survey_city == 'witchita'] <- 'wichita'

print(sort(unique(filtered$survey_city)))
```

Drop Rows with the following values : indiana, dontknowyet, na, tbd, various
```{r}
ValueToRemove = c("indiana", "dontknowyet", "na", "tbd", "various", "NA")

filtered <- filtered[!(filtered$survey_city %in% ValueToRemove), ]

unique(filtered$survey_city)
```


# The rest of your document

Add sections and subsections as necessary to guide your analysis
